%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matlab random read paralle 63G %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
>> main_call_hor
Read 1.335144e+02 MB in 0.619652 seconds: 215.466753 MB/s, across 14 chunks (random access, parallel).
speed = 2.2593e+08
time = 0.6197

Read 1.277924e+03 MB in 4.037070 seconds: 316.547294 MB/s, across 134 chunks (random access, parallel).
speed = 3.3192e+08
time = 4.0371

Read 1.275063e+04 MB in 33.117921 seconds: 385.006825 MB/s, across 1337 chunks (random access, parallel).
speed = 4.0371e+08
time = 33.1179

Read 6.371498e+04 MB in 143.800782 seconds: 443.078127 MB/s, across 6681 chunks (random access, parallel).
speed = 4.6460e+08
time = 143.8008

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matlab random read paralle 91G %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
> main_call_hor
Read 1.907349e+02 MB in 1.218775 seconds: 156.497190 MB/s, across 20 chunks (random access, parallel).

speed = 1.6410e+08
time = 1.2188
Read 1.859665e+03 MB in 5.971997 seconds: 311.397497 MB/s, across 195 chunks (random access, parallel).

speed = 3.2652e+08
time = 5.9720
Read 1.851082e+04 MB in 48.407335 seconds: 382.396975 MB/s, across 1941 chunks (random access, parallel).
speed = 4.0097e+08
time = 48.4073

Read 9.254456e+04 MB in 213.520039 seconds: 433.423280 MB/s, across 9704 chunks (random access, parallel).
speed = 4.5448e+08
time =213.5200

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matlab random read paralle 178G %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>> main_call_hor
Starting parallel pool (parpool) using the 'local' profile ...
Connected to the parallel pool (number of workers: 30).
Read 3.719330e+02 MB in 2.687909 seconds: 138.372610 MB/s, across 39 chunks (random access, parallel).
speed = 1.4509e+08
time = 2.6879

Read 3.643036e+03 MB in 9.317458 seconds: 390.990320 MB/s, across 382 chunks (random access, parallel).
speed = 4.0998e+08
time = 9.3175

Read 3.637314e+04 MB in 90.036894 seconds: 403.980378 MB/s, across 3814 chunks (random access, parallel).
speed = 4.2360e+08
time = 90.0369

Read 1.818657e+05 MB in 430.280812 seconds: 422.667447 MB/s, across 19070 chunks (random access, parallel).
speed = 4.4320e+08
time = 430.2808

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matlab read serial %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%main_call
%Size of the file "/mnt/ceph/home/y1113254/Horace_test/sqw/STO_300K_35meV.sqw" is 190692061367 bytes.

%ans =

%   1.9069e+11

%Read 190692061367 bytes in 1003.566252 seconds: 181.211874 MB/s
%Size of the file "/mnt/ceph/home/y1113254/Horace_test/sqw/STO_300K_35meV_h4.sqw" is 97036862336 bytes.

%ans =

%   9.7037e+10

%Read 97036862336 bytes in 497.804300 seconds: 185.899486 MB/s
%Size of the file "/mnt/ceph/home/y1113254/Horace_test/sqw/STO_400K_10.1meV.sqw" is 66808311941 bytes.

%ans =

%   6.6808e+10

%Read 66808311941 bytes in 169.664574 seconds: 375.525484 MB/s


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matlab read parallel %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main_call_p
%Size of the file "/mnt/ceph/home/y1113254/Horace_test/sqw/STO_300K_35meV.sqw" is 190692061367 bytes.

%ans =

   %1.9069e+11

%Starting parallel pool (parpool) using the 'local' profile ...
%Connected to the parallel pool (number of workers: 30).
%Read 190692061367 bytes in 213.855654 seconds: 850.377897 MB/s
%Size of the file "/mnt/ceph/home/y1113254/Horace_test/sqw/STO_300K_35meV_h4.sqw" is 97036862336 bytes.

%ans =

%   9.7037e+10

%Read 97036862336 bytes in 117.344840 seconds: 788.629166 MB/s
%Read 66808311941 bytes in 73.084685 seconds: 871.774589 MB/s

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Python read serial local %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
fan@UbuntuFan:~/Downloads/Horest_Test/Fan_code$ python3 readFile.py ../sqw/STO_400K_10.1meV.sqw 10 
File size: 63713.37 MB
Total time spent: 47.41 seconds.
Read speed: 1343.78 MB/s
fan@UbuntuFan:~/Downloads/Horest_Test/Fan_code$ python3 readFile.py ../sqw/STO_300K_35meV.sqw 10 
File size: 181858.12 MB
Total time spent: 134.01 seconds.
Read speed: 1357.01 MB/s

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Python read parallel local %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
fan@UbuntuFan:~/Downloads/Horest_Test/Fan_code$ python3 readFile.py ../sqw/STO_400K_10.1meV.sqw 10 --parallel
File size: 63713.37 MB
Using a pool of 4 workers for parallel processing.
Total time spent: 33.86 seconds.
Read speed: 1881.69 MB/s
fan@UbuntuFan:~/Downloads/Horest_Test/Fan_code$ python3 readFile.py ../sqw/STO_300K_35meV.sqw 10 --parallel
File size: 181858.12 MB
Using a pool of 4 workers for parallel processing.
Total time spent: 93.61 seconds.
Read speed: 1942.64 MB/s

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Python read serial DAaaS Large %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_300K_35meV.sqw 10 
File size: 181858.12 MB
Total time spent: 804.22 seconds.
Read speed: 226.13 MB/s
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_300K_35meV_h4.sqw 10 
File size: 92541.56 MB
Total time spent: 387.89 seconds.
Read speed: 238.58 MB/s
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_400K_10.1meV.sqw 10 
File size: 63713.37 MB
Total time spent: 308.15 seconds.
Read speed: 206.76 MB/s

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Python read parallel DAaaS Large %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_300K_35meV.sqw 10 --parallel
File size: 181858.12 MB
Using a pool of 30 workers for parallel processing.
Total time spent: 136.56 seconds.
Read speed: 1331.74 MB/s
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_300K_35meV_h4.sqw 10 --parallel
File size: 92541.56 MB
Using a pool of 30 workers for parallel processing.
Total time spent: 67.95 seconds.
Read speed: 1361.95 MB/s
[y1113254@host-172-16-111-243 fan_test_20240124]$ python3 readFile.py ../sqw/STO_400K_10.1meV.sqw 10 --parallel
File size: 63713.37 MB
Using a pool of 30 workers for parallel processing.
Total time spent: 46.21 seconds.
Read speed: 1378.80 MB/s